{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 00 - Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x):\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        sig = list()\n",
    "        for item in x:\n",
    "            sig = np.append(sig, 1 / (1 + math.exp(-item)))\n",
    "    else:\n",
    "        sig = 1 / (1 + math.exp(-x))\n",
    "    return(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01798620996209156\n",
      "0.8807970779778823\n",
      "[0.01798621 0.88079708 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_(-4))\n",
    "print(sigmoid_(2))\n",
    "print(sigmoid_([-4, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 01 - Logistic Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_(y_true, y_pred, m, eps=1e-15):\n",
    "    log_loss = 0.0\n",
    "    try:\n",
    "        for i in range(len(y_pred)):\n",
    "            log_loss += y_true[i] * math.log(y_pred[i]) + (1 - y_true[i]) * math.log(1 - y_pred[i])\n",
    "        return(-log_loss / len(y_true))\n",
    "    except:\n",
    "        return (-y_true * math.log(y_pred) + (1 - y_true) * math.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12692801104297263\n",
      "-10.100041078711822\n",
      "7.233347032629922\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "x = 4\n",
    "y_true = 1\n",
    "theta = 0.5\n",
    "y_pred = sigmoid_(x * theta)\n",
    "m = 1 # length of y_true is 1\n",
    "print(log_loss_(y_true, y_pred, m))\n",
    "# 0.12692801104297152\n",
    "# Test n.2\n",
    "x = [1, 2, 3, 4]\n",
    "y_true = 0\n",
    "theta = [-1.5, 2.3, 1.4, 0.7]\n",
    "x_dot_theta = sum([a*b for a, b in zip(x, theta)])\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "m = 1\n",
    "print(log_loss_(y_true, y_pred, m))\n",
    "# 10.100041078687479\n",
    "# Test n.3\n",
    "x_new = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "y_true = [1, 0, 1]\n",
    "theta = [-1.5, 2.3, 1.4, 0.7]\n",
    "x_dot_theta = []\n",
    "for i in range(len(x_new)):\n",
    "    my_sum = 0\n",
    "    for j in range(len(x_new[i])):\n",
    "        my_sum += x_new[i][j] * theta[j]\n",
    "    x_dot_theta.append(my_sum)\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "m = len(y_true)\n",
    "print(log_loss_(y_true, y_pred, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02 - Logistic Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    dot = 0.0\n",
    "    if not isinstance(x, np.ndarray) or x.size == 0 or x.ndim > 2:\n",
    "        print('Dot Error : x dim')\n",
    "        return None\n",
    "    if not isinstance(y, np.ndarray) or y.size == 0 or y.ndim > 2:\n",
    "        print('Dot Error : y dim')\n",
    "        return None\n",
    "    if x.ndim != y.ndim:\n",
    "        print('Dot Error : incompatible x and y dim')\n",
    "        return None\n",
    "    #x_t = x.reshape(-1, 1)\n",
    "    for elem in range(x.size):\n",
    "        dot += x[elem] * y[elem]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradient_(x, y_true, y_pred):\n",
    "    try:\n",
    "        grad = np.empty(0)\n",
    "        for item in x:\n",
    "            grad = np.append(grad, (y_pred - y_true) * np.array(item))\n",
    "    except :\n",
    "        grad = dot(np.array((y_pred - y_true)).reshape(-1, 1), np.array(x))\n",
    "    return (grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83201839 3.49447722]\n",
      "\n",
      "\n",
      "[ 0.99999686 -0.49999843  2.29999277 -1.49999528  3.19998994]\n",
      "\n",
      "\n",
      "[0.99994451 5.99988885 6.99983336 7.99977787 8.99972238]\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "x = [1, 4.2] # 1 represent the intercept\n",
    "y_true = 1\n",
    "theta = [0.5, -0.5]\n",
    "x_dot_theta = sum([a*b for a, b in zip(x, theta)])\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "print(log_gradient_(x, y_pred, y_true))\n",
    "# [0.8320183851339245, 3.494477217562483]\n",
    "print(\"\\n\")\n",
    "# Test n.2\n",
    "x = [1, -0.5, 2.3, -1.5, 3.2]\n",
    "y_true = 0\n",
    "theta = [0.5, -0.5, 1.2, -1.2, 2.3]\n",
    "x_dot_theta = sum([a*b for a, b in zip(x, theta)])\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "print(log_gradient_(x, y_true, y_pred))\n",
    "# [0.99999685596372, -0.49999842798186, 2.299992768716556, -1.4999952839455801, 3.1999899390839044]\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test n.3\n",
    "x_new = [[1, 2, 3, 4, 5], [1, 6, 7, 8, 9], [1, 10, 11, 12, 13]]\n",
    "# first column of x_new are intercept values initialized to 1\n",
    "y_true = [1, 0, 1]\n",
    "theta = [0.5, -0.5, 1.2, -1.2, 2.3]\n",
    "x_new_dot_theta = []\n",
    "for i in range(len(x_new)):\n",
    "    my_sum = 0\n",
    "    for j in range(len(x_new[i])):\n",
    "        my_sum += x_new[i][j] * theta[j]\n",
    "    x_new_dot_theta.append(my_sum)\n",
    "y_pred = sigmoid_(x_new_dot_theta)\n",
    "print(log_gradient_(x_new, y_true, y_pred))\n",
    "# [0.9999445100449934, 5.999888854245219, 6.999833364290213, 7.999777874335206, 8.999722384380199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03 - Vectorized Logistic Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_log_loss_(y_true, y_pred, m, eps=1e-15):\n",
    "    log_loss = 0.0\n",
    "    y_true = np.array(y_true).reshape(-1, 1)\n",
    "    y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "    for i in range(y_true.size):\n",
    "        log_loss += (y_true[i] * math.log(y_pred[i])) + ((1 - y_true[i]) * math.log(1 - y_pred[i]))\n",
    "    return(log_loss / -m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12692801]\n",
      "[10.10004108]\n",
      "[7.23334703]\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "y_true = 1\n",
    "theta = 0.5\n",
    "y_pred = sigmoid_(x * theta)\n",
    "m = 1 # length of y_true is 1\n",
    "print(vec_log_loss_(y_true, y_pred, m))\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y_true = 0\n",
    "theta = np.array([-1.5, 2.3, 1.4, 0.7])\n",
    "y_pred = sigmoid_(np.dot(x, theta))\n",
    "m = 1\n",
    "print(vec_log_loss_(y_true, y_pred, m))\n",
    "\n",
    "x_new = np.arange(1, 13).reshape((3, 4))\n",
    "y_true = np.array([1, 0, 1])\n",
    "theta = np.array([-1.5, 2.3, 1.4, 0.7])\n",
    "y_pred = sigmoid_(np.dot(x_new, theta))\n",
    "m = len(y_true)\n",
    "print(vec_log_loss_(y_true, y_pred, m)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04 - Vectorized Logistic Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    dot = 0.0\n",
    "    if not isinstance(x, np.ndarray) or x.size == 0 or x.ndim > 2:\n",
    "        print('Dot Error : x dim')\n",
    "        return None\n",
    "    if not isinstance(y, np.ndarray) or y.size == 0 or y.ndim > 2:\n",
    "        print('Dot Error : y dim')\n",
    "        return None\n",
    "    if x.ndim != y.ndim:\n",
    "        print('Dot Error : incompatible x and y dim')\n",
    "        return None\n",
    "    for elem in range(x.size):\n",
    "        dot += x[elem] * y[elem]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_log_gradient_(x, y_true, y_pred):\n",
    "    try :\n",
    "         res = dot(np.array((y_pred - y_true)).reshape(-1, 1), x)       \n",
    "    except :\n",
    "        res = (y_pred - y_true) * x\n",
    "    return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Error : incompatible x and y dim\n",
      "None\n",
      "\n",
      "Dot Error : incompatible x and y dim\n",
      "None\n",
      "\n",
      "[0.99994451 5.99988885 6.99983336 7.99977787 8.99972238]\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "x = np.array([1, 4.2]) # x[0] represent the intercept\n",
    "y_true = 1\n",
    "theta = np.array([0.5, -0.5])\n",
    "y_pred = sigmoid_(np.dot(x, theta))\n",
    "print(vec_log_gradient_(x, y_pred, y_true))\n",
    "# [0.83201839 3.49447722]\n",
    "print()\n",
    "\n",
    "\n",
    "# Test n.2\n",
    "x = np.array([1, -0.5, 2.3, -1.5, 3.2]) # x[0] represent the intercept\n",
    "y_true = 0\n",
    "theta = np.array([0.5, -0.5, 1.2, -1.2, 2.3])\n",
    "y_pred = sigmoid_(np.dot(x, theta))\n",
    "print(vec_log_gradient_(x, y_true, y_pred))\n",
    "# [ 0.99999686 -0.49999843 2.29999277 -1.49999528 3.19998994]\n",
    "print()\n",
    "\n",
    "# Test n.3\n",
    "x_new = np.arange(2, 14).reshape((3, 4))\n",
    "x_new = np.insert(x_new, 0, 1, axis=1)\n",
    "# first column of x_new are now intercept values initialized to 1\n",
    "y_true = np.array([1, 0, 1])\n",
    "theta = np.array([0.5, -0.5, 1.2, -1.2, 2.3])\n",
    "y_pred = sigmoid_(np.dot(x_new, theta))\n",
    "print(vec_log_gradient_(x_new, y_true, y_pred))\n",
    "# [0.99994451 5.99988885 6.99983336 7.99977787 8.99972238]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../subjects/day02/resources/dataset/train_dataset_clean.csv', delimiter=',', header=None, index_col=False)\n",
    "df_test = pd.read_csv('../subjects/day02/resources/dataset/test_dataset_clean.csv', delimiter=',', header=None, index_col=False)\n",
    "\n",
    "x_train, y_train = np.array(df_train.iloc[:, 1:82]), df_train.iloc[:, 0]\n",
    "x_test, y_test = np.array(df_test.iloc[:, 1:82]), df_test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBatchGd:\n",
    "    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate # can be 'constant' or 'invscaling'\n",
    "        self.thetas = []\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        y_pred = self.predict(x_train)\n",
    "        X_train = np.concatenate((np.ones((x_train.shape[0],1)), x_train), axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            if i % 150 == 0:\n",
    "                loss = self.mse(x_train, y_train)\n",
    "                print(\"epoch =\", i,\"  loss = \", loss)\n",
    "            self.thetas = self.thetas - self.alpha * vec_log_gradient_(X_train, np.array(y_train).reshape(-1, 1), y_pred)\n",
    "\n",
    "    def predict(self, x_train):\n",
    "        if not np.array(self.thetas).any():\n",
    "            self.thetas = np.ones(x_train.shape[1] + 1)\n",
    "        pred = np.empty(0)\n",
    "        X_conc = np.concatenate((np.ones((x_train.shape[0],1)), x_train), axis=1)\n",
    "        for j in range(X_conc.shape[0]):\n",
    "            pred = np.append(pred, dot(self.thetas, X_conc[j]))\n",
    "        return (pred.reshape(-1, 1))\n",
    "    \n",
    "    def score(self, x_train, y_train):\n",
    "        y_pred = (self.predict(x_train) >= 0.5).astype(int)\n",
    "        y_pred = y_pred.flatten()\n",
    "        accuracy = np.mean(y_pred == y_train)\n",
    "        return accuracy * 100\n",
    "\n",
    "    def mse(self, x_train, y_train):\n",
    "        mse_ = 0.0\n",
    "        y_pred = self.predict(x_train)\n",
    "        for i in range(y_train.size):\n",
    "            mse_ += (y_train[i] - y_pred[i])**2\n",
    "        return mse_ / y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.996007493627346\n",
      "epoch = 0   loss =  [18.49908904]\n",
      "epoch = 150   loss =  [8.18612075e+10]\n",
      "epoch = 300   loss =  [3.27449605e+11]\n",
      "epoch = 450   loss =  [7.36765192e+11]\n",
      "epoch = 600   loss =  [1.30980797e+12]\n",
      "epoch = 750   loss =  [2.04657794e+12]\n",
      "epoch = 900   loss =  [2.94707509e+12]\n",
      "epoch = 1050   loss =  [4.01129944e+12]\n",
      "epoch = 1200   loss =  [5.23925098e+12]\n",
      "epoch = 1350   loss =  [6.6309297e+12]\n",
      "None\n",
      "75.54129172936949\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionBatchGd(alpha=0.01, max_iter=1500, verbose=True, learning_rate='constant')\n",
    "print(model.score(x_train, y_train))\n",
    "print(model.fit(x_train, y_train))\n",
    "print(model.score(x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
