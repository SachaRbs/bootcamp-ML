{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 00 - Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x):\n",
    "    x = np.asarray(x)\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01798620996209156\n",
      "0.8807970779778823\n",
      "[0.01798621 0.88079708 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_(-4))\n",
    "print(sigmoid_(2))\n",
    "print(sigmoid_([-4, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 01 - Logistic Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_(y_true, y_pred, m, eps=1e-15):\n",
    "    log_loss = 0.0\n",
    "    y_true = np.array(y_true).reshape(-1, 1)\n",
    "    y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "    for i in range(len(y_pred)):\n",
    "        log_loss += y_true[i] * math.log(y_pred[i]) + (1 - y_true[i]) * math.log(1 - y_pred[i])\n",
    "    return(-log_loss / len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12692801]\n",
      "[10.10004108]\n",
      "[7.23334703]\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "x = 4\n",
    "y_true = 1\n",
    "theta = 0.5\n",
    "y_pred = sigmoid_(x * theta)\n",
    "m = 1 # length of y_true is 1\n",
    "print(log_loss_(y_true, y_pred, m))\n",
    "# 0.12692801104297152\n",
    "# Test n.2\n",
    "x = [1, 2, 3, 4]\n",
    "y_true = 0\n",
    "theta = [-1.5, 2.3, 1.4, 0.7]\n",
    "x_dot_theta = sum([a*b for a, b in zip(x, theta)])\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "m = 1\n",
    "print(log_loss_(y_true, y_pred, m))\n",
    "# 10.100041078687479\n",
    "# Test n.3\n",
    "x_new = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "y_true = [1, 0, 1]\n",
    "theta = [-1.5, 2.3, 1.4, 0.7]\n",
    "x_dot_theta = []\n",
    "for i in range(len(x_new)):\n",
    "    my_sum = 0\n",
    "    for j in range(len(x_new[i])):\n",
    "        my_sum += x_new[i][j] * theta[j]\n",
    "    x_dot_theta.append(my_sum)\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "m = len(y_true)\n",
    "print(log_loss_(y_true, y_pred, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02 - Logistic Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    dot = 0.0\n",
    "    if not isinstance(x, np.ndarray) or x.size == 0 or x.ndim > 2:\n",
    "        print('Dot Error : x dim')\n",
    "        return None\n",
    "    if not isinstance(y, np.ndarray) or y.size == 0 or y.ndim > 2:\n",
    "        print('Dot Error : y dim')\n",
    "        return None\n",
    "    if x.ndim != y.ndim:\n",
    "        print('Dot Error : incompatible x and y dim')\n",
    "        return None\n",
    "    #x_t = x.reshape(-1, 1)\n",
    "    for elem in range(x.size):\n",
    "        dot += x[elem] * y[elem]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradient_(x, y_true, y_pred):\n",
    "    \n",
    "#     try:\n",
    "    grad = np.empty(0)\n",
    "    for item in x:\n",
    "#         print(item)\n",
    "#         print(y_pred - y_true)\n",
    "        grad = np.append(grad, np.array(y_pred - y_true).reshape(-1, 1) * np.array(item))\n",
    "#     except :\n",
    "#         grad = dot(np.array((y_pred - y_true)).reshape(-1, 1), np.array(x))\n",
    "    return (grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83201839 3.49447722]\n",
      "\n",
      "\n",
      "[ 0.99999686 -0.49999843  2.29999277 -1.49999528  3.19998994]\n",
      "\n",
      "\n",
      "[-5.54485247e-05 -1.10897049e-04 -1.66345574e-04 -2.21794099e-04\n",
      " -2.77242624e-04  9.99999959e-01  1.99999992e+00  2.99999988e+00\n",
      "  3.99999983e+00  4.99999979e+00 -3.09081649e-11 -6.18163298e-11\n",
      " -9.27244947e-11 -1.23632660e-10 -1.54540825e-10 -5.54485247e-05\n",
      " -3.32691148e-04 -3.88139673e-04 -4.43588198e-04 -4.99036723e-04\n",
      "  9.99999959e-01  5.99999975e+00  6.99999971e+00  7.99999967e+00\n",
      "  8.99999963e+00 -3.09081649e-11 -1.85448989e-10 -2.16357154e-10\n",
      " -2.47265319e-10 -2.78173484e-10 -5.54485247e-05 -5.54485247e-04\n",
      " -6.09933772e-04 -6.65382297e-04 -7.20830821e-04  9.99999959e-01\n",
      "  9.99999959e+00  1.09999995e+01  1.19999995e+01  1.29999995e+01\n",
      " -3.09081649e-11 -3.09081649e-10 -3.39989814e-10 -3.70897979e-10\n",
      " -4.01806144e-10]\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "x = [1, 4.2] # 1 represent the intercept\n",
    "y_true = 1\n",
    "theta = [0.5, -0.5]\n",
    "x_dot_theta = sum([a*b for a, b in zip(x, theta)])\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "print(log_gradient_(x, y_pred, y_true))\n",
    "# [0.8320183851339245, 3.494477217562483]\n",
    "print(\"\\n\")\n",
    "# Test n.2\n",
    "x = [1, -0.5, 2.3, -1.5, 3.2]\n",
    "y_true = 0\n",
    "theta = [0.5, -0.5, 1.2, -1.2, 2.3]\n",
    "x_dot_theta = sum([a*b for a, b in zip(x, theta)])\n",
    "y_pred = sigmoid_(x_dot_theta)\n",
    "print(log_gradient_(x, y_true, y_pred))\n",
    "# [0.99999685596372, -0.49999842798186, 2.299992768716556, -1.4999952839455801, 3.1999899390839044]\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test n.3\n",
    "x_new = [[1, 2, 3, 4, 5], [1, 6, 7, 8, 9], [1, 10, 11, 12, 13]]\n",
    "# first column of x_new are intercept values initialized to 1\n",
    "y_true = [1, 0, 1]\n",
    "theta = [0.5, -0.5, 1.2, -1.2, 2.3]\n",
    "x_new_dot_theta = []\n",
    "for i in range(len(x_new)):\n",
    "    my_sum = 0\n",
    "    for j in range(len(x_new[i])):\n",
    "        my_sum += x_new[i][j] * theta[j]\n",
    "    x_new_dot_theta.append(my_sum)\n",
    "y_pred = sigmoid_(x_new_dot_theta)\n",
    "print(log_gradient_(x_new, y_true, y_pred))\n",
    "# [0.9999445100449934, 5.999888854245219, 6.999833364290213, 7.999777874335206, 8.999722384380199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03 - Vectorized Logistic Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_log_loss_(y_true, y_pred, m, eps=1e-15):\n",
    "    log_loss = 0.0\n",
    "    y_true = np.array(y_true).reshape(-1, 1)\n",
    "    y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "    for i in range(y_true.size):\n",
    "        log_loss += (y_true[i] * math.log(y_pred[i])) + ((1 - y_true[i]) * math.log(1 - y_pred[i]))\n",
    "    return(log_loss / -m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12692801]\n",
      "[10.10004108]\n",
      "[7.23334703]\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "y_true = 1\n",
    "theta = 0.5\n",
    "y_pred = sigmoid_(x * theta)\n",
    "m = 1 # length of y_true is 1\n",
    "print(vec_log_loss_(y_true, y_pred, m))\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y_true = 0\n",
    "theta = np.array([-1.5, 2.3, 1.4, 0.7])\n",
    "y_pred = sigmoid_(np.dot(x, theta))\n",
    "m = 1\n",
    "print(vec_log_loss_(y_true, y_pred, m))\n",
    "\n",
    "x_new = np.arange(1, 13).reshape((3, 4))\n",
    "y_true = np.array([1, 0, 1])\n",
    "theta = np.array([-1.5, 2.3, 1.4, 0.7])\n",
    "y_pred = sigmoid_(np.dot(x_new, theta))\n",
    "m = len(y_true)\n",
    "print(vec_log_loss_(y_true, y_pred, m)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04 - Vectorized Logistic Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    dot = 0.0\n",
    "    if not isinstance(x, np.ndarray) or x.size == 0 or x.ndim > 2:\n",
    "        print('Dot Error : x dim')\n",
    "        return None\n",
    "    if not isinstance(y, np.ndarray) or y.size == 0 or y.ndim > 2:\n",
    "        print('Dot Error : y dim')\n",
    "        return None\n",
    "    if x.ndim != y.ndim:\n",
    "        print('Dot Error : incompatible x and y dim')\n",
    "        return None\n",
    "    for elem in range(x.size):\n",
    "        dot += x[elem] * y[elem]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_log_gradient_(x, y_true, y_pred):\n",
    "    Y = (y_pred - y_true).reshape(-1, 1)\n",
    "    if x.ndim != 2:\n",
    "        x = np.array([x])\n",
    "    return (dot((y_pred - y_true).reshape(-1, 1), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83201839 3.49447722]\n",
      "\n",
      "[ 0.99999686 -0.49999843  2.29999277 -1.49999528  3.19998994]\n",
      "\n",
      "[0.99994451 5.99988885 6.99983336 7.99977787 8.99972238]\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "x = np.array([1, 4.2]) # x[0] represent the intercept\n",
    "y_true = 1\n",
    "theta = np.array([0.5, -0.5])\n",
    "y_pred = sigmoid_(np.dot(x, theta))\n",
    "print(vec_log_gradient_(x, y_pred, y_true))\n",
    "# [0.83201839 3.49447722]\n",
    "print()\n",
    "\n",
    "\n",
    "# Test n.2\n",
    "x = np.array([1, -0.5, 2.3, -1.5, 3.2]) # x[0] represent the intercept\n",
    "y_true = 0\n",
    "theta = np.array([0.5, -0.5, 1.2, -1.2, 2.3])\n",
    "y_pred = sigmoid_(np.dot(x, theta))\n",
    "print(vec_log_gradient_(x, y_true, y_pred))\n",
    "# [ 0.99999686 -0.49999843 2.29999277 -1.49999528 3.19998994]\n",
    "print()\n",
    "\n",
    "# Test n.3\n",
    "x_new = np.arange(2, 14).reshape((3, 4))\n",
    "x_new = np.insert(x_new, 0, 1, axis=1)\n",
    "# first column of x_new are now intercept values initialized to 1\n",
    "y_true = np.array([1, 0, 1])\n",
    "theta = np.array([0.5, -0.5, 1.2, -1.2, 2.3])\n",
    "y_pred = sigmoid_(np.dot(x_new, theta))\n",
    "print(vec_log_gradient_(x_new, y_true, y_pred))\n",
    "# [0.99994451 5.99988885 6.99983336 7.99977787 8.99972238]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../subjects/day02/resources/dataset/train_dataset_clean.csv', delimiter=',', header=None, index_col=False)\n",
    "df_test = pd.read_csv('../subjects/day02/resources/dataset/test_dataset_clean.csv', delimiter=',', header=None, index_col=False)\n",
    "\n",
    "x_train, y_train = np.array(df_train.iloc[:, 1:82]), df_train.iloc[:, 0]\n",
    "x_test, y_test = np.array(df_test.iloc[:, 1:82]), df_test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBatchGd:\n",
    "    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate # can be 'constant' or 'invscaling'\n",
    "        self.thetas = []\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        y_pred = self.predict(x_train)\n",
    "        X_train = np.concatenate((np.ones((x_train.shape[0],1)), x_train), axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "#             if i % 150 == 0:\n",
    "#                 loss = self.loss_(x_train, y_train)\n",
    "#                 print(\"epoch =\", i,\"  loss = \", loss)\n",
    "#             print(self.__vec_log_gradient_(x_train, y_train, y_pred))\n",
    "#             print(_vec_log_gradient(x_train, y_train, y_pred))\n",
    "#             self.thetas -= self.alpha / m * gradient\n",
    "            self.thetas = self.thetas - self.alpha * vec_log_gradient_(X_train, np.array(y_train).reshape(-1, 1), y_pred)\n",
    "        return(self.thetas)\n",
    "    def predict(self, x_train):\n",
    "        if not np.array(self.thetas).any():\n",
    "            self.thetas = np.ones(x_train.shape[1] + 1)\n",
    "        X_conc = np.concatenate((np.ones((x_train.shape[0],1)), x_train), axis=1)\n",
    "        pred = np.empty(0)\n",
    "        for j in range(X_conc.shape[0]):\n",
    "            pred = np.append(pred, sigmoid_(dot(self.thetas, X_conc[j])))\n",
    "        return (pred.reshape(-1, 1))\n",
    "    \n",
    "    def __vec_log_gradient_(self, x, y_true, y_pred):\n",
    "        X = np.array(x)\n",
    "        y_t, y_p = np.array(y_true), np.array(y_pred)\n",
    "        # gradient = np.dot(X.T, (y_p - y_t)) / y_t.shape[0]\n",
    "        # gradient = ((y_p - y_t) * X.T) / X.shape[0]\n",
    "        gradient = np.dot((y_p - y_t), X)\n",
    "        return gradient\n",
    "    \n",
    "    def score(self, x_train, y_train):\n",
    "        y_pred = (self.predict(x_train) >= 0.5).astype(int)\n",
    "        y_pred = y_pred.flatten()\n",
    "        accuracy = np.mean(y_pred == y_train)\n",
    "        return accuracy * 100\n",
    "\n",
    "    def loss_(self, x_train, y_train, eps=1e-15):\n",
    "        y_pred = self.predict(x_train)\n",
    "        m = y_pred.shape[0]\n",
    "        log_loss = 0.0\n",
    "        y_true = np.array(y_train).reshape(-1, 1)\n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        for i in range(y_true.size):\n",
    "            log_loss += (y_true[i] * math.log(y_pred[i])) + ((1 - y_true[i]) * math.log(1 - y_pred[i]))\n",
    "        return(log_loss / -m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionBatchGd(alpha=0.01, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.15585864e+02,  1.84659255e+01,  2.59035060e+01,  2.82351692e+01,\n",
       "        1.54345252e+01,  1.33920495e+01, -1.60375189e+01, -1.30808358e+01,\n",
       "       -7.90181846e+00, -3.67921392e+00, -4.67059308e+00,  8.68593898e-01,\n",
       "        9.59440000e-01, -8.91170942e+01, -3.78307159e+01, -8.18714606e+00,\n",
       "       -7.87994950e+00, -2.67823750e+00,  8.85505303e-01, -2.57157691e+01,\n",
       "       -1.86426500e+01, -2.96983829e+01, -2.26622694e+01, -2.65642456e+01,\n",
       "       -1.39883843e+01, -9.93555573e+00, -9.38946282e+00, -6.81378735e+00,\n",
       "       -4.93679091e+00, -3.01335983e+00, -3.51289513e-01,  9.26849497e-01,\n",
       "       -7.02046129e+01, -4.04477291e+01, -3.02231826e+01, -6.40532129e+00,\n",
       "       -7.46859401e+00, -2.49758623e+01, -6.33539057e+00, -1.57980026e+00,\n",
       "       -1.26444204e+00, -8.91558340e+01, -3.75529819e+00, -3.23494694e-01,\n",
       "        1.18196282e-01,  3.50104084e-02,  2.29251599e-01,  1.98278960e-01,\n",
       "        4.18886220e-01,  3.46279949e-01,  4.34391133e-01,  4.69160313e-01,\n",
       "        3.75443822e-01,  3.02166338e-01,  5.67153003e-01,  3.53654877e-01,\n",
       "        6.27307143e-01,  4.88406753e-01,  5.51610462e-01,  4.07883963e-01,\n",
       "        4.56994820e-01,  6.13225204e-01,  7.17056838e-01,  6.96945680e-01,\n",
       "        7.55819301e-01,  7.09589109e-01,  7.96294647e-01,  7.44359399e-01,\n",
       "        7.78924112e-01,  8.35462078e-01,  8.19467392e-01,  8.69916406e-01,\n",
       "        8.56779044e-01,  8.90763414e-01,  8.37247926e-01,  8.63167898e-01,\n",
       "        9.06317963e-01,  8.47511515e-01,  9.16979531e-01,  8.95276884e-01,\n",
       "        9.04085777e-01,  9.90000259e-01])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.05602164e-166],\n",
       "       [4.20544708e-106],\n",
       "       [7.24569932e-154],\n",
       "       ...,\n",
       "       [2.81924866e-159],\n",
       "       [7.27691827e-191],\n",
       "       [6.88901948e-121]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBatchGd:\n",
    "    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate # can be 'constant' or 'invscaling'\n",
    "        self.thetas = []\n",
    "        self.threshold = .5\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        m, n = x_train.shape[:2]\n",
    "        self.thetas = np.zeros(n)\n",
    "        gap = int(self.max_iter / 10)\n",
    "        for i in range(self.max_iter):\n",
    "            y_pred = self.__sigmoid_(np.dot(x_train, self.thetas))\n",
    "            self.thetas -= self.alpha / m * self.__vec_log_gradient_(x_train, y_train, y_pred)\n",
    "\n",
    "            if self.verbose and i % 150 == 0:\n",
    "                y_pred = self.__sigmoid_(np.dot(x_train, self.thetas))\n",
    "                print(f'epoch: {i} loss: {self.__vec_log_loss_(y_train, y_pred, y_train.shape[0])} \\t')\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_train):\n",
    "        return self.__sigmoid_(np.dot(x_train, self.thetas)) >= self.threshold\n",
    "\n",
    "    def score(self, x_train, y_train):\n",
    "        return (self.predict(x_train) == y_train).mean()\n",
    "\n",
    "    def __sigmoid_(self, x):\n",
    "        x = np.asarray(x)\n",
    "        sigm = 1. / (1. + np.exp(-x))\n",
    "        return sigm\n",
    "\n",
    "    def __vec_log_gradient_(self, x, y_true, y_pred):\n",
    "        X = np.array(x)\n",
    "        y_t, y_p = np.array(y_true), np.array(y_pred)\n",
    "        gradient = np.dot((y_p - y_t), X)\n",
    "        return gradient\n",
    "\n",
    "    def __vec_log_loss_(self, y_true, y_pred, m, eps=1e-15):\n",
    "        y_t, y_p = np.array(y_true), np.array(y_pred)\n",
    "        cost = (1 / m) * (((-y_t).T * np.log(y_p + eps)) - ((1 - y_t).T * np.log(1 - y_p + eps)))\n",
    "        return cost if isinstance(cost, float) else cost.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionBatchGd(alpha=0.01, max_iter=1500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.6919184362942151 \t\n",
      "epoch: 150 loss: 0.5664406156483319 \t\n",
      "epoch: 300 loss: 0.5038371783940814 \t\n",
      "epoch: 450 loss: 0.46685696168771085 \t\n",
      "epoch: 600 loss: 0.4424623865617848 \t\n",
      "epoch: 750 loss: 0.4251654907630811 \t\n",
      "epoch: 900 loss: 0.4122715670402301 \t\n",
      "epoch: 1050 loss: 0.4022999198252241 \t\n",
      "epoch: 1200 loss: 0.3943668357591168 \t\n",
      "epoch: 1350 loss: 0.38791103922603876 \t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegressionBatchGd at 0x1059b8f50>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on train dataset : 0.831669789011394\n",
      "Score on test dataset : 0.8328726736686936\n"
     ]
    }
   ],
   "source": [
    "print(f'Score on train dataset : {model.score(x_train, y_train)}')\n",
    "y_pred = model.predict(x_test)\n",
    "print(f'Score on test dataset : {(y_pred == y_test).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 06 - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy_score_(y_true, y_pred):\n",
    "    return ((y_pred == y_true).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "# Test n.1\n",
    "y_pred = np.array([1, 1, 0, 1, 0, 0, 1, 1])\n",
    "y_true = np.array([1, 0, 0, 1, 0, 1, 0, 0])\n",
    "print(accuracy_score_(y_true, y_pred))\n",
    "# print(accuracy_score(y_true, y_pred))\n",
    "# 0.5\n",
    "\n",
    "\n",
    "# Test n.2\n",
    "y_pred = np.array(['norminet', 'dog', 'norminet', 'norminet', 'dog', 'dog', 'dog', 'dog'])\n",
    "y_true = np.array(['dog', 'dog', 'norminet', 'norminet', 'dog', 'norminet', 'dog', 'norminet'])\n",
    "print(accuracy_score_(y_true, y_pred))\n",
    "# print(accuracy_score(y_true, y_pred))\n",
    "# 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 07 - Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score_(y_true, y_pred, pos_label=1):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-3174b677f197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norminet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norminet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norminet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norminet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norminet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norminet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norminet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# print(precision_score(y_true, y_pred, pos_label='dog'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-402-4126124bed51>\u001b[0m in \u001b[0;36mprecision_score_\u001b[0;34m(y_true, y_pred, pos_label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprecision_score_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# # Test n.1\n",
    "# y_pred = np.array([1, 1, 0, 1, 0, 0, 1, 1])\n",
    "# y_true = np.array([1, 0, 0, 1, 0, 1, 0, 0])\n",
    "# print(precision_score_(y_true, y_pred))\n",
    "# # print(precision_score(y_true, y_pred))\n",
    "# # 0.4\n",
    "# # 0.4\n",
    "\n",
    "# Test n.2\n",
    "y_pred = np.array(['norminet', 'dog', 'norminet', 'norminet', 'dog', 'dog', 'dog', 'dog'])\n",
    "y_true = np.array(['dog', 'dog', 'norminet', 'norminet', 'dog', 'norminet', 'dog', 'norminet'])\n",
    "print(precision_score_(y_true, y_pred, pos_label='dog'))\n",
    "# print(precision_score(y_true, y_pred, pos_label='dog'))\n",
    "# 0.6\n",
    "# 0.6\n",
    "# # Test n.3\n",
    "# print(precision_score_(y_true, y_pred, pos_label='norminet'))\n",
    "# print(precision_score(y_true, y_pred, pos_label='norminet'))\n",
    "# 0.6666666666666666\n",
    "# 0.6666666666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
